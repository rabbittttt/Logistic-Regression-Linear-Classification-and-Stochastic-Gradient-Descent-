{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-2-32e8c3288b64>, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-32e8c3288b64>\"\u001b[1;36m, line \u001b[1;32m27\u001b[0m\n\u001b[1;33m    return 1.0 / (1 + exp(-x))\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "#代码尚在修改中\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "X_train, Y_train = load_svmlight_file('a9a.txt')\n",
    "X_test, Y_test = load_svmlight_file('data2.txt')\n",
    "\n",
    "X_train = np.asarray(scipy.sparse.csr_matrix(X_train).todense())\n",
    "Y_train = np.asarray(scipy.sparse.csr_matrix(Y_train).todense())\n",
    "X_test = np.asarray(scipy.sparse.csr_matrix(X_test).todense())\n",
    "Y_test = np.asarray(scipy.sparse.csr_matrix(Y_test).todense())\n",
    "\n",
    "#参数初始化\n",
    "feature_num=123\n",
    "batch_size=128\n",
    "SGD_methods=[\"SGD\",\"NAG\",\"RMSProp\",\"AdaDelta\",\"Adam\"]\n",
    "parm={\"SGD\":{\"learning rate\":0.01},\\\n",
    "      \"NAG\":{\"learning rate\":0.01,\"Gamma\":0.9},\\\n",
    "      \"RMSProp\":{\"learning rate\":0.01,\"Gamma\":0.9,\"Epsilon\":10e-8},\\\n",
    "      \"AdaDelta\":{\"Gamma\":0.95,\"Epsilon\":10e-6},\\\n",
    "      \"Adam\":{\"Beta\":0.9,\"Gamma\":0.999,\"learning rate\":0.1,\"Epsilon\":10e-8}}\n",
    "\n",
    "\n",
    "#the sigmoid function\n",
    "def sigmoid(x):\n",
    "return 1.0 / (1 + exp(-x))\n",
    "\n",
    "\n",
    "#the cost function\n",
    "def costfunction(y,h):\n",
    "y = array(y)\n",
    "h = array(h)\n",
    "J = sum(y*log(h))+sum((1-y)*log(1-h))\n",
    "return J\n",
    "\n",
    "\n",
    "# the batch gradient descent algrithm\n",
    "def gradescent(x,y):\n",
    "m,n = shape(x)     #m: number of training example; n: number of features\n",
    "x = c_[ones(m),x]     #add x0\n",
    "x = mat(x)      # to matrix\n",
    "y = mat(y)\n",
    "a = 0.002       # learning rate\n",
    "maxcycle = 2000\n",
    "theta = ones((n+1,1))    #initial theta\n",
    "\n",
    "J = []\n",
    "for i in range(maxcycle):\n",
    "    h = sigmoid(x*theta)\n",
    "    theta = theta + a * x.transpose()*(y-h)\n",
    "    cost = costfunction(y,h)\n",
    "    J.append(cost)\n",
    "\n",
    "plt.plot(J)\n",
    "plt.show()\n",
    "return theta,cost\n",
    "\n",
    "\n",
    "#the stochastic gradient descent (m should be large,if you want the result is good)\n",
    "def stocGraddescent(x,y):\n",
    "m,n = shape(x)     #m: number of training example; n: number of features\n",
    "x = c_[ones(m),x]     #add x0\n",
    "x = mat(x)      # to matrix\n",
    "y = mat(y)\n",
    "a = 0.01       # learning rate\n",
    "theta = ones((n+1,1))    #initial theta\n",
    "\n",
    "J = []\n",
    "for i in range(m):\n",
    "    h = sigmoid(x[i]*theta)\n",
    "    theta = theta + a * x[i].transpose()*(y[i]-h)\n",
    "    cost = costfunction(y,h)\n",
    "    J.append(cost)\n",
    "plt.plot(J)\n",
    "plt.show()\n",
    "return theta,cost\n",
    "\n",
    "\n",
    "#plot the decision boundary\n",
    "def plotbestfit(x,y,theta):\n",
    "plt.plot(x[:,0:1][where(y==1)],x[:,1:2][where(y==1)],'ro')\n",
    "plt.plot(x[:,0:1][where(y!=1)],x[:,1:2][where(y!=1)],'bx')\n",
    "x1= arange(-4,4,0.1)\n",
    "x2 =(-float(theta[0])-float(theta[1])*x1) /float(theta[2])\n",
    "\n",
    "plt.plot(x1,x2)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel(('x2'))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def classifyVector(inX,theta):\n",
    "prob = sigmoid(sum(inX*theta))\n",
    "print 'the probobility is:',prob\n",
    "if prob > 0.5:\n",
    "    return 1.0\n",
    "else:\n",
    "    return 0.0\n",
    "\n",
    "if __name__=='__main__':\n",
    "x,y = loadData(\"testSet.txt\")\n",
    "theta,cost = gradescent(x,y)\n",
    "print 'theta:\\n',theta\n",
    "print 'J:',cost\n",
    "\n",
    "def LinearClassificationModel():\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    for method in SGD_methods:\n",
    "        W = np.random.rand(features_num + 1, 1)\n",
    "        iter_ = []\n",
    "        error = []\n",
    "        num = 0\n",
    "        for j in range(2):\n",
    "            for i in range(0, int(data_size / batch_size ) + 1):\n",
    "                iter_.append(num)\n",
    "                X,y=get_sub_batch(i,X_train,y_train,data_size)\n",
    "                W=opitimizer(W,X,y,method)\n",
    "                error.append(compute_loss(W,X_test,y_test))\n",
    "                num+=1\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
